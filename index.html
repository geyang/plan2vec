<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Plan2Vec: Unsupervised Representation Learning by Latent Plans</title>
    <meta name="description"
          content="we introduce plan2vec, an unsupervised representation learning
          approach that is inspired by reinforcement learning. Plan2vec constructs
          a weighted graph on an image dataset using near-neighbor distances, and
          then extrapolates this local metric to a global embedding by distilling
          path-integral over planned path. When applied to control, plan2vec offers
          a way to learn goal-conditioned value estimates that are accurate over
          long horizons that is both compute and sample efficient. We demonstrate
          the effectiveness of plan2vec on one simulated and two challenging real-world
          image datasets. Experimental results show that plan2vec successfully
          amortizes the planning cost, enabling reactive planning that is linear
          in memory and computation complexity rather than exhaustive over the entire
          state space.">
    <meta name="keywords" content="Contrastive Learning, Deep Reinforcement Learning, Universal Value Function">
    <meta name="author" content="Ge Yang <ge.ike.yang@gmail.com>">
    <meta property="og:title" content="Plan2Vec: Unsupervised Representation Learning by Latent Plans">
    <meta property="og:image" content="https://geyang.github.io/plan2vec/thumbnail.jpg">
    <meta name="twitter:creator" content="@episodeyang">
    <meta name="twitter:card" content="summary">
    <meta property="og:description"
          content="we introduce plan2vec, an unsupervised representation learning
          approach that is inspired by reinforcement learning. Plan2vec constructs
          a weighted graph on an image dataset using near-neighbor distances, and
          then extrapolates this local metric to a global embedding by distilling
          path-integral over planned path. When applied to control, plan2vec offers
          a way to learn goal-conditioned value estimates that are accurate over
          long horizons that is both compute and sample efficient. We demonstrate
          the effectiveness of plan2vec on one simulated and two challenging real-world
          image datasets. Experimental results show that plan2vec successfully
          amortizes the planning cost, enabling reactive planning that is linear
          in memory and computation complexity rather than exhaustive over the entire
          state space.">
    <style>
        body {
            margin: 0;
            width: 100%;
            position: absolute;
            left: 0;
            right: 0;
            font-size: 17px;
        }

        article {
            font-family: Georgia, Times New Roman, Times, serif;
            font-size: 100%;

            display: grid;
            align-items: center;

            margin: 0;
            padding-top: min(200px, 12%);
            padding-bottom: 12%;

            line-height: 1.5em;
        }

        #frontmatter {
            display: contents
        }

        #frontmatter > h1,
        #frontmatter > h2,
        #frontmatter > h3 {
            text-align: center;
            grid-column: 2 / span 3;
        }

        #frontmatter h3 {
            font-size: 100%;
            font-weight: 100;
        }

        #frontmatter p {
            grid-column: 3 / span 1;
        }
        #frontmatter iframe {
            grid-column: 3 / span 1;
        }
        article {
            grid-template-columns: 1fr 140px fit-content(600px) 140px 1fr;
            grid-column-gap: 20px;
        }
        @media only screen and (max-width: 781px) {
            article {
                grid-template-columns: 1fr 0 fit-content(600px) 0 1fr;
                grid-column-gap: 20px;
            }
            #frontmatter p {
                grid-column: 2 / span 3;
            }
            #frontmatter iframe {
                grid-column: 2 / span 3;
            }
        }
        @media only screen and (min-width: 781px) and (max-width: 959px) {
            article {
                grid-template-columns: 1fr 1fr fit-content(600px) 1fr 1fr;
                grid-column-gap: 20px;
            }
            #frontmatter p {
                grid-column: 3 / span 1;
            }
            #frontmatter iframe {
                grid-column: 3 / span 1;
            }
        }
        #authors a {
            color: inherit;
            text-decoration: underline;
        }
        #authors a:hover {
            color: #345cc1;
        }
        #links {
            font-family: 'Consolas', 'Deja Vu Sans Mono', 'Bitstream Vera Sans Mono', monospace;
            font-size: 115% !important;
            color: #9f9f9f;
        }
        #links a {
            color: #345cc1;
            padding: 0 0.25em;
        }
        #links a:hover {
            text-decoration: underline;
        }

        h1 {
            font-size: 150%;
        }

        h2 {
            font-size: 110%
        }

        h3, h4, h5, h6, p {
            font-size: 100%;
        }

        h2, h3, p {
            text-align: justify;
            text-justify: inter-word;
            grid-column: 2 / span 3;
        }

        pre {
            line-break: anywhere;
            white-space: pre-wrap;
            word-wrap: break-word;
            grid-column: 2 / span 3;
            transition: all 1s;
            padding: 1em 0;
            border-radius: 7px;
            font-size: 95%;
        }
        pre:hover {
            background: #eee;
        }

        sup {
            position: relative;
            left: -0.25em;
            margin-right: -0.25em;
        }

        iframe.video {
            margin: 1em 0;
        }

        a {
            color: #345cc1;
            text-decoration: none;
        }

    </style>
</head>
<body>
<article>
    <section id="frontmatter">
        <h1>Plan2Vec: Unsupervised Representation Learning by Latent Plans</h1>
        <h2 id="authors" style="margin-bottom: 0;">Ge Yang,<sup>*†</sup> Amy Zhang,<sup>*†</sup>
            Ari S. Morcos,<sup>†</sup> Joelle Pineau,<sup>†§</sup> Pieter Abbeel,<sup>‡</sup>
            <a href="https://www.robertocalandra.com">Roberto Calandra</a><sup>†</sup></h2>
        <h3 style="margin-top: 10px;"><sup>*</sup>Equal Contribution, <sup>†</sup>Facebook AI Research,
            <sup>§</sup>McGill University, <sup>‡</sup>UC Berkeley</h3>
        <h3 id="links">
            <a href="https://github.com/geyang/plan2vec">CODE</a
            >|<a href="https://arxiv.org/abs/2005.03648">PAPER</a
            >|<a href="https://github.com/geyang/plan2vec/blob/slides/Plan2vec%203min%20slides.pdf">SLIDES</a>
        </h3>
        <h2>Overview</h2>
        <p>Plan2vec is an unsupervised representation learning method that uses <i>graph-search</i> to learn
            <i>long-horizon relationships</i> between images.</p>
        <iframe class="video" width="100%" height="338px" src="https://www.youtube.com/embed/Eod6Cns2P9U?rel=0"
                frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                allowfullscreen></iframe>
    </section>
    <h2 id="abstract">Abstract</h2>
    <p>In this paper we introduce plan2vec, an unsupervised representation learning approach that is inspired by
        reinforcement learning. Plan2vec constructs a weighted graph on an image dataset using near-neighbor
        distances,
        and then extrapolates this local metric to a global embedding by distilling path-integral over planned path.
        When applied to control, plan2vec offers a way to learn goal-conditioned value estimates that are accurate
        over
        long horizons that is both compute and sample efficient. We demonstrate the effectiveness of plan2vec on one
        simulated and two challenging real-world image datasets. Experimental results show that plan2vec
        successfully
        amortizes the planning cost, enabling reactive planning that is linear in memory and computation complexity
        rather than exhaustive over the entire state space.</p>
    <h2>BibTex</h2>
    <pre>@inproceedings{yang2020plan2vec,
    title={Plan2vec: Unsupervised Representation Learning by Latent Plans},
    author={Yang, Ge and Zhang, Amy and Morcos, Ari S. and Pineau, Joelle
            and Abbeel, Pieter and Calandra, Roberto},
    booktitle={Proceedings of The 2nd Annual Conference on Learning for Dynamics and Control},
    series={Proceedings of Machine Learning Research},
    pages={1-12},
    year={2020},
    volume={120},
    note={arXiv:2005.03648}
}</pre>
</article>
</body>
</html>